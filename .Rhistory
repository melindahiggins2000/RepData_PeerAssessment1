packages()
x <- c(1,2,3,4,5,6,7,8,9,10)
y <- sin(x)
plot(x,y)
line(x,y)
Title
========================================================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).
When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
summary(cars)
```
You can also embed plots, for example:
```{r fig.width=7, fig.height=6}
plot(cars)
```
summary(cars)
plot(cars)
summary(cars)
plot(cars)
Title
========================================================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).
When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
summary(cars)
```
You can also embed plots, for example:
```{r fig.width=7, fig.height=6}
plot(cars)
```
# R script for running the Rep-Res_Example1
#
# Melinda Higgins
# dated 06/30/2014
# -----------------------------------------------
system("cd /Rep-Res-ExampleProject1-master/Rep-Res-ExampleProject1-master/Data; make")
pwd
cwd
getwd()
system("cd c:/Rep-Res-ExampleProject1-master/Rep-Res-ExampleProject1-master/Data; make")
library()
install.packages(c("googleVis", "MASS", "Matrix", "mgcv"))
url <- "http://bit.ly/S0vxk2"
temp <- tempfile()
download.file(url, temp)
install.packages("gdata")
load gdata
load()
?load
?library
library(gdata)
library(gdata)
ls()
#############
# Download and clean Pemstein (2010) Unified Democracy Score Data
# Christopher Gandrud
# Updated 17 February 2013
# Data downloaded from http://www.unified-democracy-scores.org/
#############
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
?gzfile
usdatatemp <- gzfile("C:\Rep-Res-ExampleProject1\Data\temp\uds_summary.csv.gz","uds_summary.csv")
usdatatemp <- gzfile('C:\Rep-Res-ExampleProject1\Data\temp\uds_summary.csv.gz',"uds_summary.csv")
usdatatemp <- gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv")
fix(usdatatemp)
gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv")
usdatatemp
class(usdatatemp)
usdatatemp <- read.csv(gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv"))
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
################
# Download Agricultural methane emissions (% of total) from WDI
# Christopher Gandrud
# Updated 7 January 2013
# For more information see: http://data.worldbank.org/indicator
################
# Load WDI
library(WDI)
library(plyr)
# Note: Fertilizer consumption/hectare of arable land indicator number:
# AG.CON.FERT.ZS
# Note: for simplicity that this example does not include
# all of the clean up  procedures covered in Chapter 7 of "Reproducible Research"
# Gather agricultural methane emissions data from WDI
FertConsumpData <- WDI(indicator = "AG.CON.FERT.ZS")
# Rename variable = year, value = FertilizerConsumption
FertConsumpData <- plyr::rename(x = FertConsumpData,
replace = c(AG.CON.FERT.ZS
= "FertilizerConsumption"))
ls()
#############
# Download and clean Pemstein (2010) Unified Democracy Score Data
# Christopher Gandrud
# Updated 17 February 2013
# Data downloaded from http://www.unified-democracy-scores.org/
#############
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
ls()
UDSData
usdatatemp
ls()
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
# Delete the temporary file.
unlink(temp)
# Created standardized country ID numbers based on iso 2 character codes
UDSData$iso2c <- countrycode(UDSData$country,
origin = "country.name",
destination = "iso2c")
# Keep only country, year, iso2c, and mean (UDS) variables
UDSData <- UDSData[, c("iso2c", "year", "mean")]
# Rename mean -> UDS
UDSData <- plyr::rename(UDSData, c("mean" = "UDS"))
USData
sessioninfo()
sessionInfo()
library()
%%%%%%%%%%%%%% Article Preamble %%%%%%%%%%%%%%
\documentclass{article}
%% Load LaTeX packages
\usepackage{url}
\usepackage{hyperref}
\usepackage[authoryear]{natbib}
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
library(ggplot2)
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
library(ggplot2)
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
plot(x=cars$speed,y=cars$dist,
xlabel="Speed(mph)",ylabel="Stopping Distance (ft)",
cex=1.5)
plot(x=cars$speed,y=cars$dist,
xlab="Speed(mph)",ylab="Stopping Distance (ft)",
cex=1.5)
warnings()
plot(x=cars$speed,y=cars$dist,xlab="Speed(mph)",ylab="Stopping Distance (ft)",cex=1.5)
?plot
?kernlab
?data
?str
# Reproducible Research
# Coursera course - Johns Hopkins - Roger Peng
#
# dated 07/08/2014 - M. Higgins
# ================================================
# load kernlab package
library (kernlab)
install.packages("kernlab")
library (kernlab)
data(spam)
str(spam[ ,1:5])
fix(spam)
# split dataset into a training and a test dataset
# perform the subsampling
# use a reproducible random seed
set.seed(3435)
# generate binomial random number 0 and 1 50/50 split of the data assigned at random
# to the training or test datasets
trainIndicator = rbinom(4601, size=1, prob=0.5)
# look at how the random split was achieved
table(trainIndicator)
# create the training dataset
trainSpam = spam[trainIndicator == 1, ]
# create the test dataset
testSpam = spam[trainindicator ==0, ]
# create the test dataset
testSpam = spam[trainIndicator ==0, ]
names(trainSpam)
# look at 1st 5 rows
head(trainSpam)
#make a frequency summary table of the training dataset - type variable
table(trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
# data is very skewed - lots of zeros (zero-inflated)
# take the log10 of the capitalAve variable - add 1 to
# adjust for the zeros - cannot take the log10 (of 0)
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
# let's now look at a matrix scatterplot of pairs of the variables
# specifically the 1st 4 columns, 1st 4 variables
# in the Spam dataset
# look at which variables, them matrix plot them
names(trainSpam[, 1:4])
plot(log10(trainSpam[, 1:4] + 1))
# let's explore the predictor space a little more
# run clustering across 57 variables
#
# creates a dendrogram
hCluster - hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
library()
?hclust
# creates a dendrogram
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
# not very helpful - but sensitive to skewness
# may want to transform log10 and run again
hClusterUpdated = hclust(dist(tlog10((trainSpam[, 1:57]))))
plot(hClusterUpdated)
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:57]))))
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:57] + 1))))
plot(hClusterUpdated)
plot(hClusterUpdated)
# actual exercise only ran 1st 55 columns
hClusterUpdated2 = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated2)
# move on into statistical, predictive modeling
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x,y) sum(x != (y>0.5))
cvError = rep(NA,55)
library(boot)
for(1 in 1:55){
lmFormula = reformulate(names(trainSpam[i], response="numType"))
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
for(i in 1:55){
lmFormula = reformulate(names(trainSpam[i], response="numType"))
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
for(i in 1:55){
lmFormula = reformulate(names(trainSpam)[i], response="numType")
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
# which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)]
# look at best predictor - assess uncertainty
# use the best model from the group
predictionModel = glm(numType ~ charDollar, family="binomial",data=trainSpam)
# get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam",dim(testSpam)[1])
# classify as 'spam' for those with prob > 0.5
predictedDpam[predictionModel$fitted > 0.5]="spam"
# classify as 'spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5]="spam"
fix(predictionTest)
fix(predictionTest$fitted)
view(predictTest)
fix(predictionModel$fitted)
# classification table
table(predictedSpam, testSpam$type)
# calculate error rate
(61 + 458)/(1346+458+61+449)
# My First Knitr Document
Melinda Higgins, Ph.D.
## Introduction
This is some text. Here is a code chunk.
```{r simulation echo=FALSE}
set.seed(1)
x <- rnorm(100)
mean(x)
```
set.seed(1)
x <- rnorm(100)
mean(x)
?print
?print/xtable
?print.xtable
?xtable
getwd()
setwd("F:/EmoryLaptopBackup/Coursera/ReproducibleResearch_July2014/PeerAssessment01_due20July2014_4.30pmEST/RepData_PeerAssessment1")
activityData <- read.csv("../data/activity.csv"", header=TRUE)
)
")"
activityData <- read.csv("../data/activity.csv", header=TRUE)
getwd()
?read.csv
activityData <- read.csv("/data/activity.csv", header=TRUE)
activityData <- read.csv("data/activity.csv", header=TRUE)
is.dataframe(activityData)
?class
class(activityData)
names(activityData)
dim(activityData)
dim(activityData)(2)
dim(activityData)[2]
table(activityData$date)
table(activityData$date,activityData$interval)
table(acticityData$date)
table(activityData$date)
as.data.frmae(table(activityData$date))
as.data.frame(table(activityData$date))
as.data.frame(table(activityData$date))$freq
as.data.frame(table(activityData$date))[,2]
dim(as.data.frame(table(activityData$date)))[1]
sum(is.na(activityData$steps))
dim(activityData[activityData$steps==0,])[1]
head(activityData)
head(activityData[activityData$steps==0,])
head(activityData[steps==0,])
activityData[activityData$steps==0,]
table(activityData$steps)
table(activityData$steps==0)
sum(activityData$steps==0)
sum(activityData$steps[activityData$steps==0])
activityData$steps[activityData$steps==0]
dim(activityData$steps[activityData$steps==0])
class(activityData$steps)
?as.vector
sum(as.vector(activityData$steps==0))
aa <- as.vector(activityData$steps==0)
class(aa)
sum(aa)
table(aa)
?sum
fix(aa)
sum(aa,na.rm=TRUE)
sum(as.vector(activityData$steps==0),na.rm=TRUE)
sum(as.vector(activityData$steps>0),na.rm=TRUE)
min(as.vector(activityData$steps>0),na.rm=TRUE)
min(activityData$steps[activityData$steps>0,"steps"])
min(activityData[,"steps"])
min(activityData[activityData$steps,"steps"])
min(activityData[activityData$steps>0,"steps"])
aa <- activityData[activityData$steps>0,"steps"]
dim(aa)
aa <- activityData[activityData$steps>0,c("steps")]
dim(aa)
class(aa)
is.data.frame(aa)
is.matrix(aa)
is.vector(aa)
length(aa)
aa <- activityData[(activityData$steps>0 & activityData$steps != na),c("steps")]
aa <- activityData[(activityData$steps>0 & activityData$steps != "NA"),c("steps")]
length(aa)
aa <- activityData[activityData$steps != "NA"]
?subset
aa <- activityData[is.na(activityData$steps),]
dim(aa)
aa <- activityData[!is.na(activityData$steps),]
dim(aa)
aa <- activityData[activityData$steps>0 & !is.na(activityData$steps),]
dim(aa)
min((activityData[activityData$steps>0 & !is.na(activityData$steps),])[1])
mmax((activityData[activityData$steps>0 & !is.na(activityData$steps),])[1])
max((activityData[activityData$steps>0 & !is.na(activityData$steps),])[1])
dim(aa)
names(aa)
min(aa$steps)
max(aa$steps)
hist(aa$steps)
fix(activityData)
hist(activityData$steps)
nomissing <- activityData[activityData$steps>0 & !is.na(activityData$steps),]
min(nomissing$steps)
max(nomissing$steps)
dim(activityData)
saveData
activityData <- activityData[1:5000,]
dim(activityData)
dim(as.data.frame(table(activityData$date)))[1]
dim(activityData)[1]
table(activityData$date)
as.data.frame(table(activityData$date))
?hist
hist(activityData$steps)
hist(activityData$steps,xlim=c(1,1000))
hist(activityData$steps)
hist(activityData$steps)
hist(activityData$steps,xlim=c(20,200))
hist(activityData$steps)
