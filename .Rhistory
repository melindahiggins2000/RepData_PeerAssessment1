packages()
x <- c(1,2,3,4,5,6,7,8,9,10)
y <- sin(x)
plot(x,y)
line(x,y)
Title
========================================================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).
When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
summary(cars)
```
You can also embed plots, for example:
```{r fig.width=7, fig.height=6}
plot(cars)
```
summary(cars)
plot(cars)
summary(cars)
plot(cars)
Title
========================================================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).
When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
summary(cars)
```
You can also embed plots, for example:
```{r fig.width=7, fig.height=6}
plot(cars)
```
# R script for running the Rep-Res_Example1
#
# Melinda Higgins
# dated 06/30/2014
# -----------------------------------------------
system("cd /Rep-Res-ExampleProject1-master/Rep-Res-ExampleProject1-master/Data; make")
pwd
cwd
getwd()
system("cd c:/Rep-Res-ExampleProject1-master/Rep-Res-ExampleProject1-master/Data; make")
library()
install.packages(c("googleVis", "MASS", "Matrix", "mgcv"))
url <- "http://bit.ly/S0vxk2"
temp <- tempfile()
download.file(url, temp)
install.packages("gdata")
load gdata
load()
?load
?library
library(gdata)
library(gdata)
ls()
#############
# Download and clean Pemstein (2010) Unified Democracy Score Data
# Christopher Gandrud
# Updated 17 February 2013
# Data downloaded from http://www.unified-democracy-scores.org/
#############
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
?gzfile
usdatatemp <- gzfile("C:\Rep-Res-ExampleProject1\Data\temp\uds_summary.csv.gz","uds_summary.csv")
usdatatemp <- gzfile('C:\Rep-Res-ExampleProject1\Data\temp\uds_summary.csv.gz',"uds_summary.csv")
usdatatemp <- gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv")
fix(usdatatemp)
gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv")
usdatatemp
class(usdatatemp)
usdatatemp <- read.csv(gzfile("C:/Rep-Res-ExampleProject1/Data/temp/uds_summary.csv.gz","uds_summary.csv"))
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
################
# Download Agricultural methane emissions (% of total) from WDI
# Christopher Gandrud
# Updated 7 January 2013
# For more information see: http://data.worldbank.org/indicator
################
# Load WDI
library(WDI)
library(plyr)
# Note: Fertilizer consumption/hectare of arable land indicator number:
# AG.CON.FERT.ZS
# Note: for simplicity that this example does not include
# all of the clean up  procedures covered in Chapter 7 of "Reproducible Research"
# Gather agricultural methane emissions data from WDI
FertConsumpData <- WDI(indicator = "AG.CON.FERT.ZS")
# Rename variable = year, value = FertilizerConsumption
FertConsumpData <- plyr::rename(x = FertConsumpData,
replace = c(AG.CON.FERT.ZS
= "FertilizerConsumption"))
ls()
#############
# Download and clean Pemstein (2010) Unified Democracy Score Data
# Christopher Gandrud
# Updated 17 February 2013
# Data downloaded from http://www.unified-democracy-scores.org/
#############
# Load library
library(countrycode)
library(plyr)
#### # For simplicity, store the shortened URL in an object called url.
url <- "http://bit.ly/1jXJgDh"
# Create a temporary file called 'temp' to put the zip file into.
temp <- tempfile()
# Create a temporary file called 'temp' to put the zip file into. temp <- tempfile()
# Download the compressed file into the temporary file.
download.file(url, temp)
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
ls()
UDSData
usdatatemp
ls()
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
# Decompress the file and convert it into a dataframe class object called data.
UDSData <- read.csv(gzfile(temp, "uds_summary.csv"))
# Delete the temporary file.
unlink(temp)
# Created standardized country ID numbers based on iso 2 character codes
UDSData$iso2c <- countrycode(UDSData$country,
origin = "country.name",
destination = "iso2c")
# Keep only country, year, iso2c, and mean (UDS) variables
UDSData <- UDSData[, c("iso2c", "year", "mean")]
# Rename mean -> UDS
UDSData <- plyr::rename(UDSData, c("mean" = "UDS"))
USData
sessioninfo()
sessionInfo()
library()
%%%%%%%%%%%%%% Article Preamble %%%%%%%%%%%%%%
\documentclass{article}
%% Load LaTeX packages
\usepackage{url}
\usepackage{hyperref}
\usepackage[authoryear]{natbib}
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
library(ggplot2)
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
library(ggplot2)
Plot <- qplot(cars$dist,cars$speed)+theme_bw()
print(Plot)
plot(x=cars$speed,y=cars$dist,
xlabel="Speed(mph)",ylabel="Stopping Distance (ft)",
cex=1.5)
plot(x=cars$speed,y=cars$dist,
xlab="Speed(mph)",ylab="Stopping Distance (ft)",
cex=1.5)
warnings()
plot(x=cars$speed,y=cars$dist,xlab="Speed(mph)",ylab="Stopping Distance (ft)",cex=1.5)
?plot
?kernlab
?data
?str
# Reproducible Research
# Coursera course - Johns Hopkins - Roger Peng
#
# dated 07/08/2014 - M. Higgins
# ================================================
# load kernlab package
library (kernlab)
install.packages("kernlab")
library (kernlab)
data(spam)
str(spam[ ,1:5])
fix(spam)
# split dataset into a training and a test dataset
# perform the subsampling
# use a reproducible random seed
set.seed(3435)
# generate binomial random number 0 and 1 50/50 split of the data assigned at random
# to the training or test datasets
trainIndicator = rbinom(4601, size=1, prob=0.5)
# look at how the random split was achieved
table(trainIndicator)
# create the training dataset
trainSpam = spam[trainIndicator == 1, ]
# create the test dataset
testSpam = spam[trainindicator ==0, ]
# create the test dataset
testSpam = spam[trainIndicator ==0, ]
names(trainSpam)
# look at 1st 5 rows
head(trainSpam)
#make a frequency summary table of the training dataset - type variable
table(trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
# data is very skewed - lots of zeros (zero-inflated)
# take the log10 of the capitalAve variable - add 1 to
# adjust for the zeros - cannot take the log10 (of 0)
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
# let's now look at a matrix scatterplot of pairs of the variables
# specifically the 1st 4 columns, 1st 4 variables
# in the Spam dataset
# look at which variables, them matrix plot them
names(trainSpam[, 1:4])
plot(log10(trainSpam[, 1:4] + 1))
# let's explore the predictor space a little more
# run clustering across 57 variables
#
# creates a dendrogram
hCluster - hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
library()
?hclust
# creates a dendrogram
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
# not very helpful - but sensitive to skewness
# may want to transform log10 and run again
hClusterUpdated = hclust(dist(tlog10((trainSpam[, 1:57]))))
plot(hClusterUpdated)
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:57]))))
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:57] + 1))))
plot(hClusterUpdated)
plot(hClusterUpdated)
# actual exercise only ran 1st 55 columns
hClusterUpdated2 = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated2)
# move on into statistical, predictive modeling
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x,y) sum(x != (y>0.5))
cvError = rep(NA,55)
library(boot)
for(1 in 1:55){
lmFormula = reformulate(names(trainSpam[i], response="numType"))
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
for(i in 1:55){
lmFormula = reformulate(names(trainSpam[i], response="numType"))
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
for(i in 1:55){
lmFormula = reformulate(names(trainSpam)[i], response="numType")
glmFit = glm(lmFormula, family="binomial", data=trainSpam)
cvError[i]=cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
# which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)]
# look at best predictor - assess uncertainty
# use the best model from the group
predictionModel = glm(numType ~ charDollar, family="binomial",data=trainSpam)
# get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam",dim(testSpam)[1])
# classify as 'spam' for those with prob > 0.5
predictedDpam[predictionModel$fitted > 0.5]="spam"
# classify as 'spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5]="spam"
fix(predictionTest)
fix(predictionTest$fitted)
view(predictTest)
fix(predictionModel$fitted)
# classification table
table(predictedSpam, testSpam$type)
# calculate error rate
(61 + 458)/(1346+458+61+449)
# My First Knitr Document
Melinda Higgins, Ph.D.
## Introduction
This is some text. Here is a code chunk.
```{r simulation echo=FALSE}
set.seed(1)
x <- rnorm(100)
mean(x)
```
set.seed(1)
x <- rnorm(100)
mean(x)
?print
?print/xtable
?print.xtable
?xtable
# The following code assumes a relative path where
# the data are located in a subdirectory called "/data/"
activityData <- read.csv("data/activity.csv", header=TRUE)
getwd()
setwd("F:/EmoryLaptopBackup/Coursera/ReproducibleResearch_July2014/PeerAssessment01_due20July2014_4.30pmEST/RepData_PeerAssessment1")
# The following code assumes a relative path where
# the data are located in a subdirectory called "/data/"
activityData <- read.csv("data/activity.csv", header=TRUE)
# create a copy of the dataset where the number of steps
# was NOT missing AND were > 0
nomissing <- activityData[activityData$steps>0 & !is.na(activityData$steps),]
# Create histogram (A) with all the data including 0's and NAs (missing)
hist(activityData$steps,
xlab="Number of Steps Per 5 Minute Interval",
ylab="Count",
main="Histogram of Steps in 5 Minute Intervals")
# Create histogram (B) for only the intervals >0 and no missing
hist(nomissing$steps,
xlab="Number of Steps Per 5 Minute Interval",
ylab="Count of Steps > 0",
main="Histogram of Steps in 5 Minute Intervals (Steps>0)")
table(activityData$steps)
?mean
tapply(activityData$steps, activityData$interval, mean)
?tapply
?mean
tapply(activityData$steps, activityData$interval, mean, na.rm=FALSE)
tapply(activityData$steps, activityData$interval, mean, na.rm=TRUE)
meanSteps <- tapply(activityData$steps, activityData$interval, mean, na.rm=TRUE)
class(meanSteps)
fix(meanSteps)
table(meanSteps)
library(tables)
install.packages("tables")
library(tables)
tabluar((activityData$interval)~(n=1)*(activityData$steps)*(mean + sd), data=activityData)
tabular((activityData$interval)~(n=1)*(activityData$steps)*(mean + sd), data=activityData)
tabular((interval+1)~(n=1)*(steps)*(mean + sd), data=activityData)
is.factor(activityData$interval)
tabular((as.facotr(interval)+1)~(n=1)*(steps)*(mean + sd), data=activityData)
tabular((as.factor(interval)+1)~(n=1)*(steps)*(mean + sd), data=activityData)
tabular((as.factor(interval)+1)~(n=1)*(steps)*(mean + sd), data=activityData, na.rm=TRUE)
tabular((as.factor(interval)+1)~(n=1)*(steps)*(mean + sd), data=nomissing, na.rm=TRUE)
tabular((as.factor(interval)+10)~(n=1)*(steps)*(mean + sd), data=nomissing)
tabular((as.factor(interval)+1)~(n=1)+Format(digits=2)*(steps)*(mean + sd), data=nomissing)
meanSteps <- tapply(activityData$steps, activityData$interval, mean, na.rm=TRUE)
summary(meanSteps)
names(meanSteps)
table(meanSteps)
?aggregate
aggregate(activityData$steps, activityData$interval, mean)
aggregate(activityData$steps, list(activityData$interval), mean)
aggregate(activityData$steps, list(activityData$interval), mean, na.action=na.omit)
aggregate(activityData$steps, list(activityData$interval), mean, na.rm=TRUE)
aggregate(activityData$steps, list(activityData$interval), c(mean, sd), na.rm=TRUE)
aa <- aggregate(activityData$steps, list(activityData$interval), mean, na.rm=TRUE)
class(aa)
dim(aa)
?names
names(aa) <- c("Interval","Average Number of Steps")
aa
?which.max
max(aa[,2])
which.max(aa[,2])
aa[which.max(aa[,2]),max(aa[,2])]
aa[which.max(aa[,2]),]
aa2 <- aggregate(activityData2$steps,
list(activityData2$interval, activityData2$weekday),
mean,
na.rm=TRUE)
# load chron and lattice libraries
library(chron)
library(lattice)
# Use chron() function to create chronological objects for times and days
activitydate <- chron(as.character(activityData$date),
format=c(dates="y-m-d"),
out.format=c("day months year"))
# create copy fo original dataset, add extracted dates and times
# and then extract the day of the week
activityData2 <- activityData
activityData2$date2 <- activitydate
activityData2$weekday <- weekdays(activityData2$date2)
aa2 <- aggregate(activityData2$steps,
list(activityData2$interval, activityData2$weekday),
mean,
na.rm=TRUE)
aa2
aa2[which.max([,3])]
names(aa2)
aa2[which.max([,3]),]
aa2[which.max(aa2[,3]),]
aa2 <- aggregate(activityData2$steps,
list(activityData2$interval, activityData2$weekday),
mean,
na.rm=TRUE)
names(aa) <- c("Interval","Day of Week","Average Number of Steps")
aa2[which.max(aa2[,3]),]
dim(aa2)
aa <- aggregate(activityData$steps,
list(activityData$interval),
mean,
na.rm=TRUE)
names(aa) <- c("Interval","Average Number of Steps")
aa[which.max(aa[,2]),]
aa2 <- aggregate(activityData2$steps,
list(activityData2$interval, activityData2$weekday),
mean,
na.rm=TRUE)
names(aa2) <- c("Interval","Day of Week","Average Number of Steps")
aa2[which.max(aa2[,3]),]
aa2[which.max(aa2[,3]),]
meanSteps <- tapply(activityData$steps,
activityData$interval,
meanSteps <- tapply(activityData$steps,
activityData$interval,
mean, na.rm=TRUE)
meanSteps <- tapply(activityData$steps,
activityData$interval,
mean, na.rm=TRUE)
summary(meanSteps)
library(tables)
# this sort of works but won't work with the NAs
tabular((as.factor(interval)+1)~(n=1)+Format(digits=2)*(steps)*(mean + sd), data=nomissing)
# also
aa <- aggregate(activityData$steps,
list(activityData$interval),
mean,
na.rm=TRUE)
names(aa) <- c("Interval","Average Number of Steps")
aa[which.max(aa[,2]),]
# load chron and lattice libraries
library(chron)
library(lattice)
# Use chron() function to create chronological objects for times and days
activitydate <- chron(as.character(activityData$date),
format=c(dates="y-m-d"),
out.format=c("day months year"))
# create copy fo original dataset, add extracted dates and times
# and then extract the day of the week
activityData2 <- activityData
activityData2$date2 <- activitydate
activityData2$weekday <- weekdays(activityData2$date2)
# now sort over interval and Day of week
aa2 <- aggregate(activityData2$steps,
list(activityData2$interval, activityData2$weekday),
mean,
na.rm=TRUE)
names(aa2) <- c("Interval","Day of Week","Average Number of Steps")
aa2[which.max(aa2[,3]),]
